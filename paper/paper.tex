 \documentclass[11pt,onecolumn]{article}
 \usepackage[margin=0.5in]{geometry}

\usepackage[ruled,vlined]{algorithm2e}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage[binary-units=true]{siunitx}
\usepackage{ulem}
%\usepackage{censor}
\usepackage[table]{xcolor}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tabularx}

\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{listings}
\usepackage{dingbat}
\usepackage{mathtools}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    citecolor=blue,
    filecolor=black,
    urlcolor=blue}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}


\makeatletter
  \def\footnoterule{\kern-3\p@
  \hrule \@width 2in \kern 2.6\p@} % the \hrule is .4pt high
\makeatother


\begin{document}

\newcommand{\fslspm}{FSL-SPM\xspace}
\newcommand{\fslafni}{FSL-AFNI\xspace}
\newcommand{\afnispm}{AFNI-SPM\xspace}
\newcommand{\tristan}[1]{\color{orange}\textbf{From Tristan:} #1\color{black}\xspace}
\newcommand{\camille}[1]{\color{blue}\textbf{From Camille:} #1\color{black}\xspace}
\newcommand{\ali}[2]{\color{green}\textbf{Ali:} #1\color{black}\xspace}
\newcommand{\discuss}[1]{\uwave{#1}}
\newcommand{\gk}[1]{\color{purple}#1 \textbf{-GK}\color{black}\xspace}
\newcommand{\yohan}[1]{\color{cyan!75!black} \textbf{Yohan:} #1\color{black}\xspace}
\newcommand{\yohanmod}[1]{\color{cyan!75!black} \sout{#1}\color{black}\xspace}


\title{Comparing software variability across and within fMRI analysis packages}

\author{Ali Salari$^1$, Yohan Chatelain$^1$, Alexander Bowring$^2$, Camille Maumet$^3$, Gregory Kiar$^4$, Tristan Glatard$^1$\\
  \vspace*{0.1cm}\\
  $^1$ Department of Computer-Science and Software Engineering, Concordia University, Montreal, Canada\\
  $^2$ Li Ka Shing Centre for Health Information and Discovery, Nuffield Department of Population Health,\\ Big Data Institute, University of Oxford, Oxford, UK\\
  $^3$ Inria, Univ Rennes, CNRS, Inserm, IRISA UMR 6074, Empenn ERL U 1228, Rennes, France\\
  $^4$ Center for the Developing Brain, Child Mind Institute, New York, NY, USA}

\maketitle
\begin{abstract}
  Variability has been broadly observed in functional MRI analyses as a result
  of software differences
  both between and within analytic tools. However, the
  relationship between within-tool and between-tool software variabilities for a
  given analysis is unclear. We extended a previous comparison of fMRI
  analysis software libraries (namely FSL, AFNI, and SPM) and related the
  observed differences to within-tool software variability
  simulated using Monte-Carlo arithmetic. In group analyses, we found that
  between-tool software variability was consistently larger than within-tool software
  variability simulated at the magnitude of
  machine error. In subject analyses, the same within-tool software variability
  approached between-tool software variability in some regions for some subjects.
  Interestingly, within- and between-tool software variabilities appeared moderately
  correlated ($r > 0.5$) in all tested conditions. Finally, we found that
  within-tool software variability had effects of similar magnitude to between-tool
  software variability when simulated from numerical perturbations introduced at the virtual
  precision of 17 bits --- a high but plausible level of uncertainty for
  scientific computing libraries. \tristan{Revise from latest results}
  %  \gk{What does this mean? They only have
  % precision of 17, or they use data that is 17 bits?}
  %  \yohan{agree with Greg, it is unclear and a strong assertion!}
  %  \camille{It is not entirely clear to me how this relates to the previous
  %  conclusions that BT variability was consistently larger than machine
  %  error} \tristan{Better?}
   Our findings motivate the
  continued investigation of within-tool software variability in neuroimaging, and position it
  as a possible proxy for studying some of the arbitrary between-tool software variations.
\end{abstract}

\section{Introduction}

Recent explorations of the analytical flexibility in brain imaging across
tools, platforms, or teams, have demonstrated unexpected variability in
results, even when analyzing identical data~\cite{botvinik2020variability}. Possible explanations for
such discrepancies include methodological flexibility~\cite{carp2012plurality} and software
variability, the focus of this paper. A recommendation to address this
variability is to adopt a ``multiverse" approach that analyzes the same
dataset multiple times in different software environments, and ultimately
conclude from the resulting set of outcomes. However, the range of
analytical conditions to be included in such multiverse analyses remains
poorly defined, both because of the boundless set of tools and
configurations, and that the precise causes of software variability remain
unclear. This lack of clarity is in part because fMRI analyses depend on
complex software stacks that leverage low-level libraries provided by the
operating system (e.g., mathematical functions), general scientific
computing methods (e.g., optimization toolboxes), and specific fMRI
analysis tools (e.g., spatial image normalization methods). At each level,
conceptual and implementation differences across experiments may each
create substantial variability in the analysis outcomes.

As a result of these factors, the variability resulting from the use of
different fMRI analysis tools implementing similar analytical approaches
(between-tool software variability), or different versions of the same tool,
 can reach worrying magnitudes. For instance, the
study in~\cite{bowring2019exploring} compared the results produced by the
three main fMRI analysis toolboxes, namely SPM~\cite{penny2011statistical},
AFNI~\cite{cox1996afni} and FSL~\cite{jenkinson2012fsl}, using similar
pipelines. It reported limited similarity between the activation clusters
produced by these tools, measured by Dice coefficients ranging from 0.0 to
0.769, where 0 indicates non-overlapping
clusters and 1 means identical clusters. More recently, the work
in~\cite{Li2021.12.01.470790} also showed a low similarity between the
results produced by five different tools (ABCD~\cite{feczko2021adolescent},
CCS~\cite{xu2015connectome}, CPAC~\cite{craddock2013towards}, DPARSF~\cite{yan2010dparsf},
fMRIPrep~\cite{esteban2019fmriprep}) and identified the main
factors contributing to these differences. The magnitude of the highlighted
differences suggest that between-tool software variability may be playing a critical
role in the reproducibility of fMRI analysis overall. 

The variability resulting from differences in lower-level software
libraries (within-tool software variability) has also been quantified in fMRI. The
study in~\cite{Glatard2015} mentions a low similarity between the activation clusters
produced by FSL using different versions of the GNU/Linux system, measured
by Dice coefficients ranging from 0.0 to 1.0, covering the full spectrum of
possible similarities. This variability resulted from updates in the GNU
mathematical library and can be properly simulated by introducing small
numerical perturbations on the results returned by mathematical functions~\cite{salari2021accurate}.
Here again, these observations could partly explain the differences
reported in~\cite{botvinik2020variability}.

The relationship between within- and between-tool software variability are poorly
understood, but could play an important role in the construction of
multiverse study environments. Importantly, if associations exist between
these two types of variability, they may both originate to some degree from
the inherent instability of brain activity estimation from BOLD signal
variations, and shed light on the numerical confidence of results. Under
this hypothesis, small perturbations introduced by low-level software
updates could trigger effects correlated with those created by tool
variations.

This paper investigates the relationship between numerical stability --- used
as a proxy for within-tool software variability --- and between-tool software variability
through two main questions:
\begin{enumerate}
\item what is the relative magnitude of within- and between-tool software variability, and
\item is there an association between within- and between-tool software variability.
\end{enumerate}

We address these two questions by reproducing the study in~\cite{bowring2019exploring} and
extending it with the addition of numerical perturbations of controlled
magnitude.

\section{Materials and Methods}

\subsection{fMRI analysis \& Dataset}

We replicated the analysis described as study `ds000001'
in~\cite{schonberg2012decreasing}, relying on the data publicly available
in OpenNeuro at \url{https://openneuro.org/datasets/ds000001} and using
three widely-used software packages for fMRI data processing, namely FMRIB
Software Library (FSL)~\cite{jenkinson2012fsl}, Analysis of Functional
NeuroImages (AFNI)~\cite{cox1996afni}, and Statistical Parametric
Mapping (SPM)~\cite{penny2011statistical}. We selected this dataset because
comparable analysis pipelines implemented in FSL, AFNI and SPM were already
publicly available and extensively described in~\cite{bowring2019exploring}.
Furthermore, the work in~\cite{bowring2019exploring} already evaluated the
effect of tool variability for this dataset, which we intended to
extend with the present quantification of within-tool variability.

In the selected study, 16 healthy adult subjects participated in the
balloon analog risk task~\cite{lejuez2002evaluation} to measure risk-taking
behavior over three scanning sessions~\cite{schonberg2012decreasing}. We
reused the preprocessing, first-level, and second-level analyses
implemented by~\cite{bowring2019exploring} consistently across all three
software packages. Table~\ref{table:pipeline-steps}, adapted from~\cite{bowring2019exploring},
summarizes the analytical steps in each
pipeline.


%%%%%%%%%% Summary of statstics %%%%%%%%
\setlength{\tabcolsep}{4pt}
\begin{table}[h]
  \centering
  \begin{tabular}{|c|l|c|c|c|}
    \hline
    %        \multirow{2}{*}{} & \multicolumn{1}{c}{Thresholded}& & \multicolumn{1}{c}{Unthresholded}& \\
    \multicolumn{2}{|c|}{} & FSL                                & AFNI       & SPM                     \\
    \hline
    {Preprocessing}        & {Motion Correction}                & \checkmark & \checkmark & \checkmark \\
    {}                     & {Segmentation}                     &            &            & \checkmark \\
    {}                     & {Brain Extraction (Anatomical)}    & \checkmark & \checkmark & \checkmark \\
    {}                     & {Brain Extraction (Functional)}    &            & \checkmark &            \\
    {}                     & {Intra-subject Coregistration}     & \checkmark & \checkmark & \checkmark \\
    {}                     & {Inter-subject Registration}       & \checkmark & \checkmark & \checkmark \\
    {}                     & {Analysis Voxel Size}              & \checkmark & \checkmark & \checkmark \\
    {}                     & {Smoothing}                        & \checkmark & \checkmark & \checkmark \\
    \hline
    {First-level}          & {Model Specification}              & \checkmark & \checkmark & \checkmark \\
    {}                     & {Inclusion of 6 Motion Parameters} & \checkmark & \checkmark & \checkmark \\
    {}                     & {Model Estimation}                 & \checkmark & \checkmark & \checkmark \\
    {}                     & {Contrasts}                        & \checkmark & \checkmark & \checkmark \\
    \hline
    {Second-level}         & {Model Specification}              & \checkmark & \checkmark & \checkmark \\
    {}                     & {Model Estimation}                 & \checkmark & \checkmark & \checkmark \\
    {}                     & {Contrasts}                        & \checkmark & \checkmark & \checkmark \\
    {}                     & {Second-level Inference}           & \checkmark & \checkmark & \checkmark \\
    \hline
  \end{tabular}
  \caption{Software processing steps (adapted from~\cite{bowring2019exploring}).}
  \label{table:pipeline-steps}
\end{table}

\subsection{Within-tool software variability simulation with Fuzzy Libmath}

We simulated within-tool software variability by introducing 
numerical noise in the analyses using
Fuzzy Libmath~\cite{salari2021accurate}, a version of the GNU
mathematical library (libmath) instrumented with Monte-Carlo arithmetic.
Monte-Carlo arithmetic simulates numerical errors
by introducing a controlled amount of noise in floating-point
operations through the following perturbation~\cite{Parker1997-qq}:

\begin{equation} \label{eq:mca_inexact}
  inexact(x) = x + 2^{e_x-t}\xi,
\end{equation}
where $e_x$ is the exponent in the floating-point representation of $x$,
$t$ is the virtual precision (the number of unperturbed bits in the
mantissa of $x$), and $\xi$ is a random uniform variable of
$(-\frac{1}{2}, \frac{1}{2})$. We introduced the perturbation using
Verificarlo~\cite{denis2015verificarlo}, an LLVM compiler supporting Monte-Carlo
arithmetic and other types of numerical instrumentations.

We loaded the instrumented libmath functions in the pipeline using
LD\_PRELOAD, a Linux mechanism to force-load a shared library into an
executable. This mechanism allows functions defined in Fuzzy Libmath to transparently
overload the original ones without the need to modify or recompile the
analysis pipeline.

Fuzzy Libmath introduces numerical perturbations in the values returned by
mathematical functions but not in their input values or within their
implementation. This is done by wrapping the original functions and
applying function \texttt{inexact} to their returned values.
Listing~\ref{algo:wrapper} shows an example of this wrapping for the
\texttt{log} function in single and double precision. In this wrapper, the
original function is called through \texttt{dlsym}, a function that returns
the memory address of a symbol --- in our case \texttt{RTLD\_NEXT}, the
address of the next occurrence of the function in memory. Compiling function wrappers
with Verificarlo instruments the result of the
addition between the original function output and the floating-point zero.

\lstdefinestyle{customasm}{
  belowcaptionskip=1\baselineskip,
  frame=L,
  xleftmargin=\parindent,
  language=[x86masm]Assembler,
  basicstyle=\footnotesize\ttfamily,
  commentstyle=\itshape\color{purple!40!black},
}
\lstinputlisting[caption=Sample wrapper function (C code),
  % \gk{I don't think we need this/it adds anything. Could go to a supplement
  % if we want it in a paper somewhere} \tristan{I agree it's a bit raw, but
  % I find it gives a more concrete idea of how the whole instrumentation
  % works. Otherwise we might as well keep equation (1) only, but there's
  % quite a bit of work between the equation and pipeline instrumentation. It
  % also shows how minimally invasive the instrumentation is.}
  label=algo:wrapper, style=customasm]{wrapper.c}

In~\cite{salari2021accurate}, Fuzzy Libmath was shown to accurately
simulate the effect of Linux operating system updates in structural
pre-processing pipelines of the Human Connectome Project which are largely based on FSL.
To validate our pipeline instrumentations for the present study, we first verified that non-instrumented
executions of the same pipeline on the same dataset led to identical
results. We also listed the pipeline
library dependencies using the \texttt{ldd} Linux utility and verified that
(1) the tested pipelines were dynamically linked to the GNU libmath library, and
(2) there was no alternative implementation of elementary mathematical functions in the pipeline dependencies.
Finally, we verified that the use of Fuzzy Libmath affected computational results.

\subsection{Data processing}

We measured between-tool software variability (BT) by running the pipelines
described in~\cite{bowring2019exploring} with FSL version 5.0.10, AFNI
version 18.1.09, and SPM12 version r7771 executed with GNU/Octave version
5.2. These software versions were identical to the ones used 
in~\cite{bowring2019exploring} except for SPM for which we used GNU/Octave
instead of MATLAB to enable mathematical function instrumentation using
Fuzzy Libmath. Indeed, MATLAB uses its own built-in mathematical functions,
which prevents the use of Fuzzy Libmath. In AFNI, we set the number of
threads to 7 even though AFNI executions
in~\cite{bowring2019exploring} were single-threaded. This was meant to
reduce the time overhead resulting from Fuzzy Libmath instrumentation.
% \gk{Did we verify that single-threading generally didn't lead to different
% results than 7-threading? As in, were they all ostensibly sampled from the
% same random distribution?} \ali{We verified that multithreading leads to
% different results (e.g. single-threading vs, 7-threading). However,
% single-threaded executions couldn't reproduce the original results.}
% \tristan{Fig S1 answers this comment and is refered later on}
All the analyses were conducted on the CentOS 7.3 operating system. The
computations were performed on \href{https://www.computecanada.ca}{Compute
  Canada's} Béluga cluster nodes, each with 2$\times$ Intel Gold 6148 Skylake~@~2.4~GHz
(40 cores/node) CPU and 8~GB of RAM per core. To facilitate portability and reproducibility,
we encapsulated the
above-mentioned software packages in Docker container images based on CentOS 7.3
which we converted to Singularity images to enable running on cluster nodes.

We simulated within-tool software variability (WT) by running the same analyses three
times using Fuzzy Libmath with a virtual precision of $t=53$~bits for
double-precision values and $t=24$~bits for single-precision values. These
values were chosen such that the numerical perturbation simulates machine
error. The resulting samples are equally plausible estimates of
the true
% \yohan{How can you assert that when you don't have ground truth? Would it be more safe to say "plausible estimates numerical solutions"?}
% \tristan{This does not refer to the ground truth but to the true \emph{numerical} result}
numerical result at the precision used by the pipelines. Moreover, to evaluate numerical perturbations at different magnitudes,
we repeated the FSL analyses for virtual
precisions ranging from $t=1$~bit to $t=24$~bits for single-precision
and double-precision values. We also repeated the FSL analyses for virtual
precisions ranging from $t=24$~bits to $t=53$~bits for double-precision values, having
set the virtual precision to $t=24$~bits for single-precision values.

We evaluated BT for thresholded as well as unthresholded
group-level and subject-level t-statistics maps by computing the absolute
differences of t-statistic maps across tools. For WT, we computed the
average absolute difference across the three Fuzzy Libmath samples.
Moreover, we computed WT variability for a pair of tools as the sum of WT values for each tool.

Further, from the thresholded maps, we determined regional instability
between activation clusters in the 360 regions in the Human Connectome
Project Multi-Modal Parcellation atlas version 1.0
(HCP-MMP1.0)~\cite{glasser2016multi}. For BT, we considered a region
unstable for a pair of tools if it contained activated voxels for a tool
but not for the other one. For WT, we considered a region unstable for a
pair of tools (A, B) if for tool A or tool B it contained activated voxels only for some Fuzzy Libmath
samples.
% \yohan{A voxel is activated if its intensity is $>$ 0? If so, the given definition focuses on regions with low intensities while regions with high intensities may be more unstable}
% \tristan{A voxel is activated if its value (t-stat) is above a threshold}

\section{Results}
The scripts, Docker images, and data to reproduce the results are available
in our GitHub repository at
\url{https://github.com/big-data-lab-team/fuzzy-neurotools}.


\subsection{Validation of replication}

We verified the correctness of our analyses by comparing our unperturbed
t-statistic group maps with the ones obtained
in~\cite{bowring2019exploring}. For FSL, we found
identical results. For SPM, the files (as measured by checksums) were different but differences
were visually unnoticeable. For AFNI, the activation maps were similar
overall, however, differences were noticeable visually.
The observed differences remained small (see Supplemental Material~\ref{sec:supp-repro}), and might be due to the use of
GNU/Octave vs MATLAB in SPM, and of multithreading in AFNI. We performed visual quality control of the AFNI
and SPM results for each individual subject and confirmed that T1-weighted images were
correctly skull-stripped and registered to the MNI template.

\subsection{In the group analysis, BT was larger than WT}

Table~\ref{table:pipeline-stats} presents summary statistics for variability in the
group-level t-statistics. For each tool pair (A, B), BT was
significantly larger than machine error in tool A or B, in both thresholded
and unthresholded maps (Wilcoxon signed-rank test and t-test p~\textless~$10^{-5}$ for all tests).
% \gk{we don't have sufficient sample size to say that p is that small,
% so we should probably say less than $10^{-5}$ or something else we could actually
% arrive at with our number of repetitions, pipeline pairs, and samples.}
% for all tests, t-test p=0 \gk{similar comment re p value} for all tests) \camille{p is smaller than a very small p but not zero?}.
These global differences were confirmed by Bland-Altman plots showing a
clear dominance of BT over WT
(Figure~\ref{fig:unthresh-maps}-\textbf{A},\textbf{B}).
In addition, BT and WT appeared moderately correlated across voxels (Pearson's r
in [0.60, 0.62], p$< 10^{-5}$, Figure~\ref{fig:unthresh-maps}-\textbf{C}). 
% Interestingly, the correlation appears driven by a set of voxels exhibiting comparable BT and
% WT values located around the identity line in Figure~\ref{fig:unthresh-maps}D, which, however, did not seem to have
% any spatial consistency.


\setlength{\tabcolsep}{5pt}
\begin{table}[h]
  \centering
  \begin{tabular}{cccccc|cc}
    \toprule
                      &              & \multicolumn{4}{c|}{Group map}  & \multicolumn{2}{c}{Subject maps}                                                                     \\
    \multirow{2}{*}{} & {}           & \multicolumn{2}{c}{Thresholded} & \multicolumn{2}{c|}{Unthresholded} & \multicolumn{2}{c}{Unthresholded}                               \\
    % \cmidrule{3-8}
    {}                & {}           & $\mu$                           & $\sigma$                           & $\mu$                             & $\sigma$ & $\mu$ & $\sigma$ \\
    \midrule
    \rowcolor{lightgray!50}
    {Between Tools}   & FSL vs. SPM  & 2.564                           & 1.050                              & 0.887                             & 0.689    & 0.733 & 0.586    \\
    \rowcolor{lightgray!50}
    {(BT)}            & FSL vs. AFNI & 3.096                           & 1.232                              & 1.095                             & 0.882    & 0.878 & 0.704    \\
    \rowcolor{lightgray!50}
    {}                & AFNI vs. SPM & 2.951                           & 1.344                              & 1.218                             & 0.955    & 0.983 & 0.762    \\
    {Within Tool}     & FSL          & 0.510                           & 0.697                              & 0.127                             & 0.098    & 0.120 & 0.083    \\
    {(WT)}            & SPM          & 0.367                           & 0.638                              & 0.085                             & 0.068    & 0.075 & 0.056    \\
    {}                & AFNI         & 0.637                           & 0.747                              & 0.199                             & 0.202    & 0.167 & 0.197    \\
    \bottomrule
  \end{tabular}
  \caption{Voxel-wise mean and standard deviation of BT and WT variability
    in t-statistics maps.}
  \label{table:pipeline-stats}
\end{table}

\begin{figure*}[ht]
  \centering
  % \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
  \begin{subfigure}[ht]{.9\textwidth}
    \centering
    \includegraphics[width=.9\textwidth]{figures/gl-unthresh.png}
    % \includegraphics[width=0.9\textwidth]{figures/abs/gl-unthresh.png}
    %\caption{Standard deviation of thresholded t-statistics map on template surface}
  \end{subfigure}
  \caption{\textbf{A} and \textbf{B}: Bland-Altman plots comparing group-level differences computed between tools
    (\textbf{A}) and within tools at machine error (\textbf{B}). \textbf{C}: voxel-wise comparison of absolute differences.}
  \label{fig:unthresh-maps}
  % \end{minipage}}
\end{figure*}


\subsection{In subject analyses, WT approached BT for some subjects}

Table~\ref{table:pipeline-stats} also presents summary statistics for
subject-level unthresholded t-statistics maps. As for group-level maps,
BT was consistently larger than WT (Wilcoxon
signed-rank test and t-test p~\textless~$10^{-5}$ for all tests).
However, for
some subjects and for AFNI, WT approached 
BT in some regions (Figure~\ref{fig:unthresh-maps-sbj}). As for the group maps, BT and WT
 appeared moderately correlated for all subjects (r in [0.484,
    0.633], p$< 10^{-5}$ for all subjects, see Supplemental Material~\ref{sec:supp-subjects} and~\ref{sec:supp-worst-subject}).

%%%%%%%%%% Var. of Unthresh sbj05%%%%%%%%
\begin{figure*}[ht]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/plots/Bland-Altman/unthresh-sbj05.png}
  % \includegraphics[width=0.9\textwidth]{figures/abs/sbj05-abs.png}
  %\caption{Standard deviation of thresholded t-statistics map on template surface}
  \caption{For subject with highest WT variability,
    unthresholded subject-level variability computed between tools (\textbf{A}), and within tools at machine error (\textbf{B}).}
    % and difference between them (\textbf{C}). \camille{If the changes are visible, I think that it would be useful to add the correspondant unthresholded maps for the reader to see where the differences are and what they "look like"}}
  \label{fig:unthresh-maps-sbj}
\end{figure*}

\subsection{At precision t=17~bits and in FSL, WT approached BT in the group analysis}
\tristan{We cannot say that from the data that we have.}
\tristan{Discuss subject-level BA plots.}
While the previous results were obtained by simulating WT with numerical perturbations 
at the magnitude of machine error, we also
evaluated numerical variability across different virtual precisions for FSL
and found that the virtual precision of t=17 bits minimized the RMSE
between BT and WT in unthresholded group maps.
FSL produced unthresholded group maps with $\mu=0.483$ and $\sigma=0.410$ at this virtual precision,
which was still lower than BT variability
(Wilcoxon signed-rank test and t-test p~\textless~$10^{-5}$ for all tests).
However, Figure~\ref{fig:gnp-mni} shows that
BT and WT reached comparable magnitudes in some regions. BT and WT remained
moderately correlated at this precision (Pearson's r
in [0.53, 0.61], p$< 10^{-5}$, Figure~\ref{fig:gnp-mni}-\textbf{B}).

%%%%%%%%%% plot at precision t=17%%%%%%%%
\begin{figure*}[ht]
  \begin{subfigure}[ht]{\textwidth}
    \centering
    \includegraphics[width=.55\textwidth]{figures/p17-diffs.pdf}
    % \includegraphics[width=.75\textwidth]{figures/bg_global_precision.pdf}
    %\caption{Standard deviation of thresholded t-statistics map on template surface}
  \end{subfigure}
  %\caption{Standard deviation of thresholded t-statistics map on template surface}
  \caption{Unthresholded group t-statistics differences computed
  within tools at the virtual precision of t=17~bits (\textbf{A}),
  and voxel-wise comparison of absolute differences between BT and WT (\textbf{B}).}
  \label{fig:gnp-mni}
\end{figure*}


\subsection{Previous results were confirmed in thresholded group maps}

We also compared BT and WT in thresholded maps since these maps are commonly used instead of unthresholded ones 
to conclude on the activation of specific regions. Thresholding is an unstable operation
that introduced variability at the edges of
active regions for both BT and WT. Except at the edges, BT remained consistently larger
than WT (Figure~\ref{fig:thresh-maps}-\textbf{A},\textbf{B},\textbf{C} and Table~\ref{table:pipeline-stats}). 

Moreover, to compare the effects of BT and WT on the detection of activated
regions, we measured WT instability and BT instability in each region of
the HCP-MMP1.0 parcellation. The confusion matrices in
Figure~\ref{fig:thresh-maps}-\textbf{D} report these instabilities for the
360 tested regions. The average ratio of unstable regions was 26\% for BT
and 9.2\% for WT, which confirmed that BT was larger than
WT even in thresholded maps.

Finally, we measured agreement between BT and WT from the confusion
matrices since interpreting the correlation of thresholded maps is
difficult due to the discontinuity introduced by thresholding. 
The average Cohen's kappa score\footnote{$\kappa \leq 0$ denotes chance agreement, $-1 \leq \kappa \leq 1$}
computed from the confusion matrices between WT instability and BT instability was $\kappa=0.17$, indicating
a moderate agreement between WT instability and BT instability consistent with the correlations observed in 
unthresholded maps.

% \gk{again, it feels kind of abrupt and I'm not really sure what this
% section is getting at or how it differs from the previous based on these
% descriptions. Perhaps introduce that thresholding is common, but since it
% makes things discontinuous we study it through aparcellation, etc...}.
% \tristan{better now?}

%%%%%%%%%% Var. of Thresh %%%%%%%%
\begin{figure*}[ht]
  \begin{subfigure}[ht]{\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figures/act_deact-marked.pdf}
    % \includegraphics[width=\textwidth]{figures/plots/wt-tstats/act_deact-framed.png}
    % \includegraphics[width=\textwidth]{figures/abs/gl-thresh.png}
    %\caption{Standard deviation of thresholded t-statistics map on template surface}
  \end{subfigure}
  \centering
  \caption{\textbf{A},\textbf{B},\textbf{C}: Thresholded group-level
    t-statistics within tools at machine error for FSL (\textbf{A}), SPM
    (\textbf{B}) and AFNI (\textbf{C}). Arrows point to activation clusters
    impacted by both within- and between-tool variability \tristan{Ali: is it really the case? Can you make the arrows point more accurately to 
    the clusters? Some of them are quite far from the cluster they point to.}\ali{Yes, it's the case.}.
     \textbf{D}: Confusion matrices of
    activation instability in BT and WT among the 360 regions of the
    HCP-MMP1.0 parcellation.}
  \label{fig:thresh-maps}
\end{figure*}


\section{Discussion}

In fMRI group analyses, within-tool software variability remains an order
of magnitude smaller than between-tool variability. This is likely
explained by the fact that group analyses benefit from regularization of
numerical noise, which is expected to increase with sample size. This finding is
consistent with observations made in~\cite{kiar2020numerical} from
diffusion MRI data where connectome graph statistics were found to be
substantially unstable at the subject level while group distributions
remained consistent. Therefore,
for fMRI studies with large sample sizes, within-tool software variability
may be neglected with respect to between-tool variability. In particular,
multiverse analyses aggregating the outcome of multiple analysis tools are
likely to successfully correct for machine error in such group studies.  Nevertheless, within-tool software variability remains
substantial in group analyses that are based on a single tool, as is
commonly the case in current fMRI studies. In particular, in our study, the
inherent instability of thresholding was triggered by within-tool software
variability in 9\% of 360 brain regions, which indicates that it might have
impacted neuroscientific conclusions related to these regions.

In subject-level analyses, within- and between-tool software variabilties can become
of comparable magnitude for some subjects in some regions. This observation
is particularly relevant to the development of fMRI-based biomarkers aiming
at individualized phenotype predictions. Machine error may play
a non-negligible role in such analyses, even when predictions combine
results produced by multiple tools.

For both group- and subject-level analyses, between- and within-tool
software variabilities were found to be moderately correlated. Even though
both types of variability are different in nature, this result suggests
that in some cases they may have a common cause that might be related to
the conditioning of fMRI analysis in a specific dataset. Along these lines,
in some regions, instabilities of similar magnitude may be triggered by
small numerical perturbations, model variations, or implementation
differences. For instance, high-motion datasets may trigger between- and
within-tool instabilities similarly. 

Therefore, numerical stability may be a suitable proxy to study
between-tool variability. This speculation might be of practical value to
address software variability at large given that within-tool variability
measurements can be automated while between-tool ones require substantial
human intervention. Indeed, within-tool variability can be simulated
irrespective of the particular analysis done, and with important sample
sizes, while between-tool variability requires re-implementing analysis in
different toolboxes and is limited to a few samples. Automated multi-verse
frameworks could be developed to assess and possibly reduce within-tool
variability as a pre-requisite for between-tool variability.

\tristan{This paragraph needs to be revised as we can't conclude that from the data we have}
The finding that numerical variability approached tool variability at the
virtual precision of t=17~bits is interesting too. Indeed, while machine
error generally introduces perturbations on the least-significant bit of the mantissa,
ones 
% \yohan{I would not mix virtual precision and error measurement}\tristan{better?}
common scientific software dependencies introduce larger perturbations
.
For instance, SciPy's 2D spline interpolation was recently found to be
precise up to 10~bits~\cite{pytracer}
and its replacement by a more precise implementation might therefore introduce
numerical perturbations leading to errors in the range of between-tool
variability. Such high errors
might be triggered by updates in operating systems, Python, MATLAB, and
other software dependencies.
% \yohan{The sentence is ok, just a picky point: the SciPy result you cited
% describes the precision of the spline implementation while at the end
% you're talking differences due to updates. The nuance is that two versions
% may have a similar precision and so when you udpate your system, the
% difference between both versions is low but each version can be
% ill-conditionned.}
% \tristan{better?}

Our results are limited by the type of numerical noise introduced in the
analyses. Indeed, we only perturbed the outputs of elementary
mathematical functions while numerical noise could creep in any
floating-point operation. Therefore, our estimation of within-tool software variability should
be considered a lower bound. Likewise, our estimation of tool varability is
likely to be underestimated, having tested only 3 analytical pipelines
among the thousands available~\cite{carp2012plurality}.

In conclusion, our results motivate further numerical stability
investigations in fMRI analyses. Pipeline-level analyses could be conducted
to identify specific components that contribute to numerical variability,
and if possible correct them accordingly. Besides, in cases where
instability is inherent to the analysis, sampling results distributions
through numerical perturbations might improve stability, as explored
in~\cite{kiar2021data}. Finally, pipeline-specific statistical corrections
might be envisaged to account for within-tool software variability.

\bibliographystyle{plain}
\bibliography{biblio}

\clearpage

\setcounter{equation}{0}
\setcounter{figure}{0}
\setcounter{table}{0}
\setcounter{section}{0}

\makeatletter
\renewcommand{\theequation}{S\arabic{equation}}
\renewcommand{\thefigure}{S\arabic{figure}}
\renewcommand{\thesection}{S\arabic{section}}

\textbf{\centering \Large Supplemental Materials}

\section{Reproduced results}
\label{sec:supp-repro}

Figure~\ref{fig:replication-diff} shows the difference in SPM and AFNI
group analyses maps between the results in~\cite{bowring2019exploring} and
our replication. Numerical perturbations of 1 ulp are likely to have been
introduced by our replication due to the use of GNU/Octave vs MATLAB for SPM
and multithreading for AFNI.
 \camille{Would this be feasible to check?}
 \tristan{Ali, could you answer that?}
 \ali{Yes, all the output data are available in Beluga cluster in `~/projects/def-glatard/between-tool-samples/results/ds000001/'.
 These replicated results can find in particular in `tools/AFNI/replicated' for AFNI and `tools/SPM/Octave' for SPM results.}
  However, the group maps remained very similar
overall, which led us to conclude that our results correctly reproduced
the ones in~\cite{bowring2019exploring}.
\begin{figure*}[ht]
  % \fbox{\begin{minipage}{\dimexpr \textwidth-2\fboxsep-2\fboxrule}
  % \centering
  \includegraphics[width=\textwidth]{figures/replication-diffs.png}
  \caption{Differences between reproduced and original results obtained in~\cite{bowring2019exploring}
    of unthresholded group-level t-statistics for SPM (left) and AFNI
    (right). The highest areas of difference in AFNI seem to be due to
    differences in brain masks.}
  \label{fig:replication-diff}
  % \end{minipage}}
\end{figure*}

\section{BT and WT correlations for all subjects}
\label{sec:supp-subjects}

Figure~\ref{fig:unthresh-correlation-allsbj} plots the relationship between
WT and BT for each subject, showing a consistent moderate correlation between BT and
WT.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=.55\textwidth]{figures/sbj-abs-corr-unthresh-plot.png}
  % \includegraphics[width=.6\textwidth]{figures/sbj-abs-corr-unthresh-plot.png}
  \caption{Comparison between BT and WT variabilty for 16 subjects.}
  \label{fig:unthresh-correlation-allsbj}
\end{figure*}

\section{Maps of t-statistics for subject with highest WT variability}
\label{sec:supp-worst-subject}

Figure~\ref{fig:unthresh-worst-sbj} plots the maps of t-statistics
for each run of tools for the subject with highest WT variability.

\begin{figure*}[ht]
  \centering
  \includegraphics[width=\textwidth]{figures/5-marked.pdf}
  % \includegraphics[width=\textwidth]{figures/plots/wt-tstats/5-framed.png}
  % \includegraphics[width=.6\textwidth]{figures/sbj-abs-corr-unthresh-plot.png}
  \caption{For subject with highest WT variability, unthresholded subject-level t-statistics
  within tools at machine error for FSL (\textbf{A}),
  SPM (\textbf{B}), AFNI (\textbf{C}).}
  \label{fig:unthresh-worst-sbj}
\end{figure*}

\end{document}
\endinput
